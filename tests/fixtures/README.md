# Test Fixtures

This directory contains test data for the PostgreSQL migration validation tests.

## Directory Structure

```
fixtures/
├── chatgpt/              # ChatGPT export samples
│   └── sample_conversation.json
├── claude/               # Claude export samples
│   └── sample_conversation.json
├── large/                # Large datasets for performance testing
│   └── (to be added)
├── golden_responses/     # Golden API response baselines
│   └── (to be populated by tests)
└── extract_samples.py    # Script to extract samples from real exports
```

## Sample Data

### ChatGPT Sample
- **File**: `chatgpt/sample_conversation.json`
- **Source**: Real ChatGPT export (anonymized)
- **Title**: "Transcription of conversation"
- **Messages**: 6 messages
- **Format**: ChatGPT's `mapping` format with Unix epoch timestamps

### Claude Sample
- **File**: `claude/sample_conversation.json`
- **Source**: Real Claude export (anonymized)
- **Title**: "Thoughtful Interview Questions"
- **Messages**: 4 messages
- **Format**: Claude's `chat_messages` format with ISO timestamps

## Usage in Tests

### Import Sample Data
```python
import json
from pathlib import Path

# Load ChatGPT sample
chatgpt_path = Path(__file__).parent / "fixtures/chatgpt/sample_conversation.json"
with open(chatgpt_path) as f:
    chatgpt_data = json.load(f)

# Load Claude sample
claude_path = Path(__file__).parent / "fixtures/claude/sample_conversation.json"
with open(claude_path) as f:
    claude_data = json.load(f)
```

### Use in Migration Tests
```python
def test_chatgpt_import(postgres_db):
    """Test importing ChatGPT conversations preserves data."""
    # Load sample
    sample_path = "tests/fixtures/chatgpt/sample_conversation.json"
    
    # Import via controller
    result = postgres_controller.import_json(sample_path)
    
    # Verify import
    assert result["imported_count"] == 1
    assert result["message_count"] == 6
```

## Regenerating Samples

If you need to extract new samples from updated exports:

```bash
cd tests/fixtures
python3 extract_samples.py
```

The script will find suitable conversations (3-6 messages) from your exports and save them to the appropriate directories.

## Golden Responses

The `golden_responses/` directory will be populated by tests to store baseline API responses. These are used for regression testing to ensure API compatibility is maintained.

Golden responses are generated by:
1. Running API contract tests with PostgreSQL backend
2. Capturing responses in JSON format
3. Storing in `golden_responses/` with descriptive filenames

Example golden response files:
- `GET_api_conversations.json`
- `POST_api_search.json`
- `POST_api_rag_query.json`

## Large Dataset Testing

The `large/` directory is reserved for performance testing with larger datasets. These can be:
- Synthetic data generated by `tests/utils/seed.py`
- Anonymized subsets of real exports (1000+ conversations)

**Note**: Large datasets are not committed to git due to size. They should be generated locally for performance tests.
