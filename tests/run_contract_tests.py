#!/usr/bin/env python3
"""
Contract Test Runner for CI/CD

This script runs all contract tests and provides clear exit codes for CI/CD integration.
It validates that:

1. All API responses conform to the defined contracts
2. Current API behavior matches golden snapshots  
3. Performance benchmarks are met
4. Error handling works as expected

Usage:
  python tests/run_contract_tests.py [--capture-snapshots] [--performance] [--verbose]
  
Exit codes:
  0 - All tests passed
  1 - Contract violations found
  2 - Performance regression detected  
  3 - Test execution failed
"""
import sys
import os
import json
import subprocess
import argparse
from datetime import datetime

# Add the project root to the path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def run_pytest_with_markers(markers, verbose=False):
    """Run pytest with specific markers"""
    cmd = ["python", "-m", "pytest"]
    
    if markers:
        cmd.extend(["-m", " and ".join(markers)])
    
    if verbose:
        cmd.append("-v")
    else:
        cmd.append("-q")
    
    cmd.extend([
        "--tb=short",
        "--json-report",
        "--json-report-file=test-results.json"
    ])
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result

def run_contract_validation():
    """Run contract validation tests"""
    print("ğŸ” Running contract validation tests...")
    
    result = run_pytest_with_markers(["contract"], verbose=True)
    
    if result.returncode == 0:
        print("âœ… All contract tests passed")
        return True
    else:
        print("âŒ Contract test failures detected:")
        print(result.stdout)
        if result.stderr:
            print("Errors:")
            print(result.stderr)
        return False

def run_integration_tests():
    """Run integration tests against live API"""
    print("ğŸ” Running API integration tests...")
    
    # Run specific integration test classes
    cmd = [
        "python", "-m", "pytest", 
        "tests/test_live_api_integration.py::TestLiveAPIEndpoints",
        "-v", "--tb=short"
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… All integration tests passed")
        return True
    else:
        print("âŒ Integration test failures detected:")
        print(result.stdout)
        if result.stderr:
            print("Errors:")
            print(result.stderr)
        return False

def run_performance_benchmarks():
    """Run performance benchmark tests"""
    print("ğŸ” Running performance benchmark tests...")
    
    result = run_pytest_with_markers(["performance"], verbose=True)
    
    if result.returncode == 0:
        print("âœ… All performance benchmarks passed")
        return True
    else:
        print("âŒ Performance benchmark failures detected:")
        print(result.stdout)
        if result.stderr:
            print("Errors:")
            print(result.stderr)
        return False

def validate_golden_responses():
    """Validate that all golden responses still conform to contracts"""
    print("ğŸ” Validating golden response contracts...")
    
    try:
        # Import and run the validation
        from api.contracts.golden_responses import validate_all_golden_responses
        
        is_valid = validate_all_golden_responses()
        
        if is_valid:
            print("âœ… All golden responses are contract compliant")
            return True
        else:
            print("âŒ Golden response contract violations found")
            return False
            
    except Exception as e:
        print(f"âŒ Error validating golden responses: {e}")
        return False

def capture_fresh_snapshots():
    """Capture fresh API snapshots"""
    print("ğŸ“¸ Capturing fresh API snapshots...")
    
    try:
        # Run the snapshot capture script
        result = subprocess.run([
            "python", "tests/capture_api_snapshots.py"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… API snapshots captured successfully")
            return True
        else:
            print("âŒ Failed to capture API snapshots:")
            print(result.stdout)
            print(result.stderr)
            return False
            
    except Exception as e:
        print(f"âŒ Error capturing snapshots: {e}")
        return False

def compare_with_baseline_snapshots():
    """Compare current API behavior with baseline snapshots"""
    print("ğŸ” Comparing current API behavior with baseline...")
    
    try:
        baseline_file = "tests/golden_responses/live_api_snapshots.json"
        
        if not os.path.exists(baseline_file):
            print("âš ï¸  No baseline snapshots found - run with --capture-snapshots first")
            return True  # Don't fail if no baseline exists
        
        # For now, just check that the file exists and is readable
        with open(baseline_file, 'r') as f:
            baseline_data = json.load(f)
        
        print(f"âœ… Baseline contains {len(baseline_data)} endpoint snapshots")
        
        # TODO: Implement detailed comparison logic
        # This would compare current responses with baseline and report differences
        
        return True
        
    except Exception as e:
        print(f"âŒ Error comparing with baseline: {e}")
        return False

def generate_test_report():
    """Generate a comprehensive test report"""
    print("ğŸ“„ Generating test report...")
    
    try:
        report = {
            "timestamp": datetime.now().isoformat(),
            "test_environment": {
                "python_version": sys.version,
                "working_directory": os.getcwd()
            },
            "tests_run": []
        }
        
        # Read pytest JSON report if available
        test_results_path = os.path.join("tests", "fixtures", "test-results.json")
        if os.path.exists(test_results_path):
            with open(test_results_path, 'r') as f:
                pytest_results = json.load(f)
                report["pytest_summary"] = pytest_results.get("summary", {})
        
        # Save report to fixtures
        report_path = os.path.join("tests", "fixtures", "contract_test_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Test report generated: {report_path}")
        return True
        
    except Exception as e:
        print(f"âš ï¸  Failed to generate test report: {e}")
        return True  # Don't fail the whole run for report generation

def main():
    """Main test runner"""
    parser = argparse.ArgumentParser(description="Run contract tests for API migration")
    parser.add_argument("--capture-snapshots", action="store_true", 
                       help="Capture fresh API snapshots before testing")
    parser.add_argument("--performance", action="store_true",
                       help="Include performance benchmark tests")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose output")
    
    args = parser.parse_args()
    
    print("ğŸš€ Starting Contract Test Suite")
    print("=" * 50)
    
    all_passed = True
    
    # Capture fresh snapshots if requested
    if args.capture_snapshots:
        if not capture_fresh_snapshots():
            all_passed = False
        print()
    
    # Validate golden responses against contracts
    if not validate_golden_responses():
        all_passed = False
    print()
    
    # Run contract validation tests
    if not run_contract_validation():
        all_passed = False
    print()
    
    # Run integration tests
    if not run_integration_tests():
        all_passed = False
    print()
    
    # Run performance benchmarks if requested
    if args.performance:
        if not run_performance_benchmarks():
            all_passed = False
        print()
    
    # Compare with baseline snapshots
    if not compare_with_baseline_snapshots():
        all_passed = False
    print()
    
    # Generate test report
    generate_test_report()
    
    # Final results
    print("=" * 50)
    if all_passed:
        print("ğŸ‰ All contract tests PASSED!")
        print("   API contract compliance verified")
        print("   Frontend compatibility preserved")
        if args.performance:
            print("   Performance benchmarks met")
        print("\nâœ… Safe to proceed with migration")
        sys.exit(0)
    else:
        print("ğŸ’¥ Contract test FAILURES detected!")
        print("   âŒ API contract violations found")
        print("   âš ï¸  Frontend compatibility may be broken")
        print("\nğŸ›‘ DO NOT proceed with migration until issues are resolved")
        sys.exit(1)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Test run interrupted by user")
        sys.exit(3)
    except Exception as e:
        print(f"\nğŸ’¥ Test execution failed: {e}")
        sys.exit(3)